{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Introduction to Python And Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor: Tak-Kei Lam\n",
    "\n",
    "(Readers are assumed to have a little bit programming background.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity-based algorithms\n",
    "When dealing with data, we often need to define how much a data point is similar to other data points. If we know how similar or dissimilar any two data points are, we can then divide the data points into groups where the elements in the same group are similar. and thus achieve machine learning. *Classification* and *clustering* are two groups of similarity-based machine learning algorithms. How good the classification or clustering result is almost always depends on how suitable the similarity metric is.\n",
    "\n",
    "Classification algorithms are supervised learning machine learning algorithms, whereas clustering algorithms are unsupervised. \n",
    "\n",
    "Example of classification algorithms:\n",
    "- k-nearest neighbours\n",
    "\n",
    "Examples of clustering algorithms:\n",
    "- k-means\n",
    "- Expectation-maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-nearest neighbours\n",
    "K-nearest neighbours is a supervised classification algorithm. Its procedures:\n",
    "1. Build a database of samples whose classes are known (the samples have been labelled)\n",
    "2. If we want to classify an unseen piece of data $d$, we need to:\n",
    "    1. Find the $k$ most similar samples in the database, which are known as the $k$-nearest neighbours, and then\n",
    "    2. Find the majority class of the $k$ nearest neighbours, and assign it to $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let write our simplest implementation of k-nearest neighbours in Python from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "\n",
    "def euclideanDistance(data1, data2):    \n",
    "    \"\"\" calculate sum of the squares of each column \"\"\"\n",
    "    total_distance = np.sum(np.square(data1[1] - data2[1]))\n",
    "    eucliden = np.sqrt(total_distance)\n",
    "    return eucliden\n",
    "\n",
    "\n",
    "def kNearestNeighboursPredict(trainingData, k, testData):\n",
    "    \"\"\" Inputs:\n",
    "            trainingData is an array of tuples\n",
    "            k specifies the number of nearest neighbours\n",
    "            testData is an array of tuples\n",
    "\n",
    "            a trainingData should be a tuple (class, numpy array of features)\n",
    "            so trainingData is [(class 1, numpy array of features),\n",
    "                                (class 2, numpy array of features),\n",
    "                                ...]\n",
    "\n",
    "            the format of testData is the same as trainingData\n",
    "\n",
    "        Calculates the class of the given data using Eucliden distance as the similarity metric.\n",
    "    \"\"\"\n",
    "    predicted_classes = []\n",
    "    \n",
    "    for d in testData:\n",
    "        # calculate the Euclidean distance\n",
    "        distances = [(tr, euclideanDistance(d, tr)) for tr in trainingData]\n",
    "        \n",
    "        # \n",
    "        sorted_distances = sorted(distances, key=lambda tup: tup[1])[:k]\n",
    "\n",
    "        kNeighbours = [d[0] for d in sorted_distances]        \n",
    "\n",
    "        classes = {}\n",
    "        for n in kNeighbours:\n",
    "            if n[0] not in classes:\n",
    "                classes[n[0]] = 1\n",
    "            else:\n",
    "                classes[n[0]] += 1                \n",
    "        \n",
    "        classes = sorted(classes.items(), key=lambda entry: entry[1])        \n",
    "        \n",
    "        predicted_classes.append(classes[0][0])\n",
    "        \n",
    "    return predicted_classes\n",
    "                \n",
    "trainingData = [('A', np.array([1])), ('A', np.array([2])),\n",
    "                ('B', np.array([11])), ('B', np.array([12]))]\n",
    "\n",
    "testData = [('X', np.array([13])), ('X', np.array([4]))]\n",
    "\n",
    "predictedData = kNearestNeighboursPredict(trainingData, 2, testData)\n",
    "\n",
    "print(predictedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn is more convenient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', can we figure out the Type of the Pokemon?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "pokemons = pd.read_csv(\"pokemon.csv\")\n",
    "print(pokemons['Type 1'].unique())\n",
    "\n",
    "pokemons = pokemons.sample(frac=1) # .sample(frac=1) randomize the data, and select 100% from the randomized\n",
    "\n",
    "label_column = ['Type 1']\n",
    "features_columns = ['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']\n",
    "\n",
    "pokemons_features = pokemons[features_columns]\n",
    "pokemons_label = pokemons[label_column]\n",
    "\n",
    "# normalise every columns in pokemons_features\n",
    "# pokemons_features = pokemons_features.apply(lambda x: (x - x.min())/(x.max() - x.min()))\n",
    "\n",
    "# .values convert the datastructure from pandas's dataframe into numpy's array\n",
    "\n",
    "# Split the data into training/test sets\n",
    "last_index = -int(0.20*len(pokemons_features))\n",
    "pokemons_features_train = pokemons_features[:last_index].values\n",
    "pokemons_features_test = pokemons_features[last_index:].values \n",
    "\n",
    "last_index = -int(0.20*len(pokemons_label))\n",
    "pokemons_label_train = pokemons_label[:last_index].values.flatten() # the expected labels\n",
    "pokemons_label_test = pokemons_label[last_index:].values.flatten()  # the expected labels\n",
    "\n",
    "# Create a k-nearest neighbours classifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Train the model using the training sets\n",
    "neigh.fit(pokemons_features_train, pokemons_label_train) \n",
    "\n",
    "# Make predictions using the testing set\n",
    "pokemons_label_pred = neigh.predict(pokemons_features_test) # the actual labels\n",
    "\n",
    "correct = 0.0\n",
    "for i in range(0, len(pokemons_label_test)):\n",
    "    print('expected {} VS. actual {}'.format(pokemons_label_test[i], pokemons_label_pred[i]))\n",
    "    if pokemons_label_pred[i] == pokemons_label_test[i]:\n",
    "        correct = correct+1\n",
    "print('Accuracy: {}%'.format(correct/len(pokemons_label_test) * 100))\n",
    "\n",
    "# What if we change k, and/or select a different set of features, and/or limit the number of Type?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have already noticed that the choice of the parameters such as $k$ and the features greatly affect the performance of k-nearest neighbours. Indeed, the parameters of the model are as important as the (dis)similarity metric in machine learning algorithms. Selecting the most suitable parameters is a big research topic per se."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means\n",
    "K-means is an unsupervised clustering algorithm. It is different from k-nearest neighours, but k-nearest neighours is sometimes applied to implement k-means (as shown below). The procedures of k-means are listed below:\n",
    "1. Guess $k$ cluster centres\n",
    "2. For each data point, assign it to one of the closest clusters. Here a similarity metric defining the distance betweeen a data point and a cluster centre is required.\n",
    "3. Update the centres of the clusters\n",
    "4. Repeat step 2-3 until the centres of the clusters do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------- the same functions defined for k-nearest neighbours BEGIN\n",
    "\n",
    "def euclideanDistance(data1, data2):    \n",
    "    \"\"\" calculate sum of the squares of each column \"\"\"\n",
    "    total_distance = np.sum(np.square(data1[1] - data2[1]))\n",
    "    eucliden = np.sqrt(total_distance)\n",
    "    return eucliden\n",
    "\n",
    "\n",
    "def kNearestNeighboursPredict(centres, k, testData):\n",
    "    \"\"\" Inputs:\n",
    "            centres is an array of tuples\n",
    "            k specifies the number of nearest neighbours\n",
    "            testData is an array of tuples\n",
    "\n",
    "            a centres should be a tuple (class, numpy array of features)\n",
    "            so centres is [(class 1, numpy array of features),\n",
    "                                (class 2, numpy array of features),\n",
    "                                ...]\n",
    "\n",
    "            the format of testData is the same as centres\n",
    "\n",
    "        Calculates the class of the given data using Eucliden distance as the similarity metric.\n",
    "    \"\"\"\n",
    "    predicted_classes = []\n",
    "    \n",
    "    for d in testData:\n",
    "        # calculate the Euclidean distance\n",
    "        distances = [(tr, euclideanDistance(d, tr)) for tr in centres]\n",
    "        \n",
    "        # \n",
    "        sorted_distances = sorted(distances, key=lambda tup: tup[1])[:k]\n",
    "\n",
    "        kNeighbours = [d[0] for d in sorted_distances]        \n",
    "\n",
    "        classes = {}\n",
    "        for n in kNeighbours:\n",
    "            if n[0] not in classes:\n",
    "                classes[n[0]] = 1\n",
    "            else:\n",
    "                classes[n[0]] += 1                \n",
    "        \n",
    "        classes = sorted(classes.items(), key=lambda entry: entry[1])        \n",
    "        \n",
    "        predicted_classes.append(classes[0][0])\n",
    "        \n",
    "    return predicted_classes\n",
    "# ---------------------------- the same functions defined for k-nearest neighbours END\n",
    "\n",
    "def kmeansFit(data, k):\n",
    "    # generate k random centres\n",
    "    rand_int = np.random.randint(-100, 100, 1)[0]\n",
    "    centre_values = [-rand_int, rand_int]\n",
    "     # food for thought, why do we pick the initial centre values in this way?\n",
    "    # how about  randomly generating them, say, from the range [-100, 100] instead?\n",
    "    print('initial random centre values: {}'.format(centre_values))\n",
    "\n",
    "     # centres is an array  in the form [ (classA, [numpy array of features]), (classB, [numpy array of features]), ...]\n",
    "    centres = []\n",
    "    for i, c in enumerate(centre_values):\n",
    "        centres.append((i, np.array([c])))\n",
    "\n",
    "    prev_centres = None\n",
    "\n",
    "    classes = {}\n",
    "\n",
    "    while True:\n",
    "        # assign a class to every data\n",
    "        assigned_classes = kNearestNeighboursPredict(centres, 1, data)\n",
    "        print(assigned_classes)\n",
    "\n",
    "        # store the class info as a dictionary in the following format:\n",
    "        # { class A: array of data, class B: array of data, ...}\n",
    "        for i in range(0, k):\n",
    "            classes[i] = []                              \n",
    "        for i, c in enumerate(assigned_classes):\n",
    "            data[i] = (c, data[i][1])\n",
    "            classes[assigned_classes[i]].append(data[i])\n",
    "\n",
    "        # update the centres of every cluster\n",
    "        for c, elements in classes.items():\n",
    "            sum = np.zeros((len(data[0][1])))\n",
    "            for e in elements:\n",
    "                    sum += e[1]\n",
    "            if (len(elements) > 0):\n",
    "                mean = sum / len(elements)\n",
    "                centres[c] = (c, mean)\n",
    "\n",
    "        for c in centres:\n",
    "            print('centre: {} has mean {}'.format(c[0], c[1]))\n",
    "\n",
    "         # check if the centres are updated\n",
    "        hasGreatlyChanged = False\n",
    "        if prev_centres:\n",
    "            for i, c in enumerate(centres):\n",
    "                    diff = np.sum(np.absolute(prev_centres[i][1]-centres[i][1]))\n",
    "                    print('prev: {} cur : {}', prev_centres[i][1], centres[i][1])\n",
    "                    if diff > 0.5:\n",
    "                        hasGreatlyChanged = True\n",
    "                        break\n",
    "        else:\n",
    "            hasGreatlyChanged = True\n",
    "\n",
    "        if not hasGreatlyChanged:\n",
    "            break\n",
    "\n",
    "        prev_centres = centres[0:]\n",
    "        # why do we have to do this???\n",
    "        # can we simply do: prev_centres = centres ??? \n",
    "        \n",
    "        yield classes # we haven't learnt using yield yet. Can you guess what it is used for?\n",
    "\n",
    "    #return classes \n",
    "\n",
    "# let's test our implementation\n",
    "                               \n",
    "# random data set 1 that consists of 10 integers in the range [-2, 4]\n",
    "data_1 = np.random.randint(-1, 5, 10)\n",
    "# random data set 2 that consists of 10 integers in the range [9, 15]\n",
    "data_2 = np.random.randint(9, 16, 10)\n",
    "# shuffle the concatenation of data_1 and data2\n",
    "data_array = np.concatenate((data_1, data_2))\n",
    "np.random.shuffle(data_array)\n",
    "\n",
    "# just assign a dummy class for each data (for format compatibility)\n",
    "data_array = [(0, np.array([d])) for i,d in enumerate(data_array)]\n",
    "\n",
    "iterations=0\n",
    "for predictedClasses in kmeansFit(data_array, 2):\n",
    "    print('------------------------ Iteration: {}'.format(iterations))\n",
    "    #print(predictedClasses)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,8), dpi=100)\n",
    "    for c,dataPoints in predictedClasses.items():\n",
    "        if c == 0:\n",
    "            colour = 'red'\n",
    "        elif c == 1:\n",
    "            colour = 'blue'\n",
    "\n",
    "        x = [x[1] for x in dataPoints]\n",
    "        y = np.zeros(len(x))\n",
    "        plt.title('Clustering result at iteration: '+ str(iterations))\n",
    "        plt.scatter(x, y, color=colour)\n",
    "    iterations = iterations +1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means using Euclidean distance as the similarity metric is defined in mathematics notations as follows:\n",
    "\n",
    "Given $n$ data points: $\\{d_1, d_2, d_3, ..., d_n\\}$, assign every data point $d_i$ to a cluster $C_i$ out of $k$ clusters such that $\\sum_1^k{\\sum_{d_i \\in C_j}{||d_i - \\mu _j||_2}}$ is minimum, where $\\mu _j$ is the mean of all data points in the cluster $C_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, some important and valuable knowledge from human are used to train the model:\n",
    "1. An educated guess of the initial centre values\n",
    "2. An educated guess of the number of clusters\n",
    "3. An educated guess of the stopping conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the examples of k-nearest neighbours (supvervised classification) and k-means (unsupervised clustering) that human knowledge is required in some sense, either explicit (through data labelling) or implicit (guesses of the parameters), to train the machine learning algorithms no matter they are supervised or unsupervised. Therefore the domain knowledge of a problem is actually very important.\n",
    "\n",
    "That implies some people probably still have jobs even if artificial intelligence has dominated the universe! (seems legit...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using k-means in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', can we figure out the Type of the Pokemon?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd    \n",
    "\n",
    "# Load the dataset\n",
    "pokemons = pd.read_csv(\"pokemon.csv\")\n",
    "print(pokemons['Type 1'].unique())\n",
    "\n",
    "pokemons = pokemons.sample(frac=1) # .sample(frac=1) randomize the data, and select 100% from the randomized\n",
    "\n",
    "label_column = ['Type 1']\n",
    "features_columns = ['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']\n",
    "\n",
    "pokemons_features = pokemons[features_columns]\n",
    "pokemons_label = pokemons[label_column]\n",
    "\n",
    "# normalise every columns in pokemons_features\n",
    "# pokemons_features = pokemons_features.apply(lambda x: (x - x.min())/(x.max() - x.min()))\n",
    "\n",
    "# .values convert the datastructure from pandas's dataframe into numpy's array\n",
    "\n",
    "# Split the data into training/test sets\n",
    "last_index = -int(0.20*len(pokemons_features))\n",
    "pokemons_features_train = pokemons_features[:last_index].values\n",
    "pokemons_features_test = pokemons_features[last_index:].values \n",
    "\n",
    "last_index = -int(0.20*len(pokemons_label))\n",
    "pokemons_label_train = pokemons_label[:last_index].values.flatten() # the expected labels\n",
    "pokemons_label_test = pokemons_label[last_index:].values.flatten()  # the expected labels\n",
    "\n",
    "kmeans = KMeans(n_clusters=18, random_state=0).fit(pokemons_features_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pokemons_label_pred = kmeans.predict(pokemons_features_test) # the actual labels\n",
    "\n",
    "correct = 0.0\n",
    "for i in range(0, len(pokemons_label_pred)):\n",
    "    print('Pokemon: {} actual type: {}'.format(pokemons.loc[i, 'Name'], pokemons_label_pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation-Maximization\n",
    "\n",
    "In k-means (clustering), we use a technique in which the centres are guessed, the data points are assigned the guessed centres, then the centres are updated according to the information that has been gathered thus far, and the procedure repeats until the result *converges*. This algorithm is actually a way of making educated and systematic trial-and-error guesses. There is a highly similar algorithm known as Expectation-Maximization.\n",
    "\n",
    "Expectation-Maximization is usually said to the the \"soft\" version of k-means. That means, instead assigning a data point to one and only one cluster, *a data point is assigned to all clusters with probability densities*; instead of characterising a cluster by its mean, a cluster is to be characterised by a *probability density function* or *mixtures of probability density functions*. The suitability that a data point belongs to a cluster depends on how it fits with the probability density functions of the cluster.\n",
    "\n",
    "We usually assume the probability density function is of [Gaussian](https://en.wikipedia.org/wiki/Normal_distribution).\n",
    "\n",
    "The two main steps of expectation-maximization:\n",
    "1. Expectation, the E step: for each data point $d_i$, compute the probability density of it belonging to each of the clusters give the current model $m$\n",
    "2. Maximization, the M step: update the model $m$ such that every data point can be assigned to a cluster with the greatest probability density (in other words, to make clusters further away from each other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A program is worth a thousand words. Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def estimate_mean(data, weight):\n",
    "    return np.sum(data * weight) / np.sum(weight)\n",
    "\n",
    "def estimate_std(data, weight, mean):\n",
    "    variance = np.sum(weight * (data - mean)**2) / np.sum(weight)\n",
    "    return np.sqrt(variance)\n",
    "\n",
    "# --------------------------------------------------- initialization\n",
    "np.random.seed(110) # for reproducible random results\n",
    "\n",
    "# set parameters\n",
    "red_mean = 3\n",
    "red_std = 0.8\n",
    "\n",
    "blue_mean = 7\n",
    "blue_std = 2\n",
    "\n",
    "# draw 20 samples from normal distributions with red/blue parameters\n",
    "red = np.random.normal(red_mean, red_std, size=20)\n",
    "blue = np.random.normal(blue_mean, blue_std, size=20)\n",
    "\n",
    "both_colours = np.sort(np.concatenate((red, blue))) # combine the points together\n",
    "\n",
    "# From now on, assume we don't know how the data points were generate. We also don't know which cluster a data belongs to.\n",
    "\n",
    "\n",
    "# ------------ initial guess \n",
    "# estimates for the mean\n",
    "red_mean_guess = 1.1\n",
    "blue_mean_guess = 9\n",
    "\n",
    "# estimates for the standard deviation\n",
    "red_std_guess = 2\n",
    "blue_std_guess = 1.7\n",
    "\n",
    "# ------------ expectation & maximization\n",
    "# graph\n",
    "g=[i for i in range(-10, 20)]\n",
    "\n",
    "for i in range(0, 10):\n",
    "    # just for plotting\n",
    "    plt.figure(figsize=(10, 6), dpi=100)    \n",
    "    plt.title(\"Red VS. Blue (Gaussian distribution) Take: \" + str(i))\n",
    "    red_norm = stats.norm(red_mean_guess, red_std_guess).pdf(g)\n",
    "    blue_norm = stats.norm(blue_mean_guess, blue_std_guess).pdf(g)\n",
    "    plt.scatter(g, red_norm)\n",
    "    plt.scatter(g, blue_norm)\n",
    "    plt.plot(g, red_norm, c='red')\n",
    "    plt.plot(g, blue_norm, c='blue')\n",
    "    \n",
    "\n",
    "    # how likely is a data point belong to the clusters?\n",
    "    likelihood_of_red = stats.norm(red_mean_guess, red_std_guess).pdf(both_colours)\n",
    "    likelihood_of_blue = stats.norm(blue_mean_guess, blue_std_guess).pdf(both_colours)\n",
    "\n",
    "    # new estimates of standard deviation\n",
    "    likelihood_total = likelihood_of_red + likelihood_of_blue\n",
    "    red_weight = likelihood_of_red / likelihood_total\n",
    "    blue_weight = likelihood_of_blue / likelihood_total\n",
    "\n",
    "    red_std_guess = estimate_std(both_colours, red_weight, red_mean_guess)\n",
    "    blue_std_guess = estimate_std(both_colours, blue_weight, blue_mean_guess)\n",
    "\n",
    "    # new estimates of mean\n",
    "    red_mean_guess = estimate_mean(both_colours, red_weight)\n",
    "    blue_mean_guess = estimate_mean(both_colours, blue_weight)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do you agree?\n",
    "\n",
    "Both k-means and expectation-maximization can acutally be used for semi-supervised machine learning in which only a portion of the data is labelled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
