{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Introduction to Python And Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor: Tak-Kei Lam\n",
    "\n",
    "(Readers are assumed to have a little bit programming background.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some categories of popular machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Regression\n",
    "In short and basic words, regression is about \"curve fitting\" while taking some constraints into account. The idea is to  derive a general model to describe the data using some functions (lines, curves, etc).\n",
    "\n",
    "Example:\n",
    "- Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "pokemons = pd.read_csv(\"pokemon.csv\")\n",
    "\n",
    "# Use two features: HP and Attack\n",
    "pokemons_HP = pokemons['HP']\n",
    "pokemons_Attack = pokemons['Attack']\n",
    "\n",
    "# .values convert the datastructure from pandas's dataframe into numpy's array\n",
    "\n",
    "# Split the data into training/test sets\n",
    "last_index = -int(0.20*len(pokemons_HP))\n",
    "pokemons_HP_train = pokemons_HP[:last_index].values\n",
    "pokemons_HP_test = pokemons_HP[last_index:].values \n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "last_index = -int(0.20*len(pokemons_Attack))\n",
    "pokemons_Attack_train = pokemons_Attack[:last_index].values \n",
    "pokemons_Attack_test = pokemons_Attack[last_index:].values \n",
    "\n",
    "# reshape each data set from a row into a column so that the data can be used by sklearn\n",
    "pokemons_HP_train = np.reshape(pokemons_HP_train, (-1, 1))\n",
    "pokemons_HP_test = np.reshape(pokemons_HP_test, (-1, 1))\n",
    "pokemons_Attack_train = np.reshape(pokemons_Attack_train, (-1, 1))\n",
    "pokemons_Attack_test = np.reshape(pokemons_Attack_test, (-1, 1))\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(pokemons_HP_train, pokemons_Attack_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pokemons_Attack_pred = regr.predict(pokemons_HP_test)\n",
    "\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(pokemons_Attack_test, pokemons_Attack_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(pokemons_Attack_test, pokemons_Attack_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.figure(figsize=(4,4), dpi=100)\n",
    "plt.scatter(pokemons_HP_test, pokemons_Attack_test,  color='black')\n",
    "plt.plot(pokemons_HP_test, pokemons_Attack_pred, color='blue', linewidth=3)\n",
    "plt.xlabel(\"HP\")\n",
    "plt.ylabel(\"Attack\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures of linear regression\n",
    "(The explanation considers only two variables. But the concept can be exteneded to multi-variable (easily).)\n",
    "1. Assume the data can be modelled using a straight line. What we need to do is to figure out its slope and the intercept. That is, the model is in the form: $y =b_0 + b_1x$\n",
    "2. Assume the line passes through the mean of the x-axis and the mean of the y-axis\n",
    "3. Find the distance between a data point and the mean of the x-axis (y-axis). The distance is known as the error of the data point (how far it is devriated from the model).\n",
    "4. Try to minimize the overall error for all data points by making the errors \"\"more even''. There are many methods to achieve that. For example, we can use *Ordinary Least Squares*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "Supppose we have the data: $\\{(0.9, 1), (1, 0.9), (2.1, 1.8), (1.9, 2.05)\\}$\n",
    "\n",
    "$\\bar{x} = \\text{mean}(x) = \\frac{0.9 + 1 + 2.1 + 1.9}{4} = 1.475$\n",
    "\n",
    "$\\bar{y} = \\text{mean}(y) = \\frac{1 + 0.9 + 1.8 + 2.05}{4} = 1.4375$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot some graphs to explain to idea\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [(0.9,1),(1,0.9),(2.1,1.8),(1.9,2.05)]\n",
    "\n",
    "data_x  = np.array([d[0] for d in data]) # we use numpy array for the convenience of performing operations such as (vector-vector)\n",
    "data_y  = np.array([d[1] for d in data])\n",
    "\n",
    "xmean = np.array([1.475 for i in range(0,4)])\n",
    "y = [i for i in range(0,4)]\n",
    "ymean = np.array([1.4375 for i in range(0,4)])\n",
    "x = [i for i in range(0,4)]\n",
    "\n",
    "plt.figure(figsize=(10,8), dpi=100)\n",
    "\n",
    "# ------------------------- first plot\n",
    "plt.subplot(2,2,1)\n",
    "plt.scatter(data_x, data_y,  color='black')\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(0, 3)\n",
    "plt.title('Data points')\n",
    "\n",
    "# ------------------------- second plot\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(data_x, data_y,  color='black')\n",
    "\n",
    "# plot mean of x\n",
    "plt.plot(xmean, y, label='x mean') \n",
    "# plot mean of y\n",
    "plt.plot(x, ymean, label='y mean') \n",
    "plt.legend()\n",
    "\n",
    "# plot x errror\n",
    "for i in range(0, len(data)):\n",
    "    plt.plot([data_x[i], xmean[0]], [data_y[i], data_y[i]], '--')     \n",
    "# plot y errror\n",
    "for i in range(0, len(data)):\n",
    "    plt.plot([data_x[i], data_x[i]], [data_y[i], ymean[0]], '--') \n",
    "\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(0, 3)\n",
    "plt.title('The means and their distances from the means')\n",
    "\n",
    "# ------------------------- third plot\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(data_x, data_y,  color='black')\n",
    "\n",
    "# plot mean of x\n",
    "plt.plot(xmean, y, label='x mean') \n",
    "# plot mean of y\n",
    "plt.plot(x, ymean, label='y mean') \n",
    "\n",
    "\n",
    "# plot x errror\n",
    "for i in range(0, len(data)):\n",
    "    plt.plot([data_x[i], xmean[0]], [data_y[i], data_y[i]], '--')     \n",
    "# plot y errror\n",
    "for i in range(0, len(data)):\n",
    "    plt.plot([data_x[i], data_x[i]], [data_y[i], ymean[0]], '--') \n",
    "    \n",
    "# plot linear regression guesses\n",
    "x = np.linspace(0,4,4)\n",
    "y = (x - xmean[0])/((3-xmean[0])/(3-ymean[0]))+ymean[0]\n",
    "plt.plot(x, y, '--', label='guess 1')\n",
    "y = (x - xmean[0])/((2-xmean[0])/(3-ymean[0]))+ymean[0]\n",
    "plt.plot(x, y, '--', label='guess 2')\n",
    "y = (x - xmean[0])/((2-xmean[0])/(1.5-ymean[0]))+ymean[0]\n",
    "plt.plot(x, y, '--', label='guess 3')\n",
    "plt.text(2.5, 2.75, '?')\n",
    "plt.text(1.75, 2.75, '?')\n",
    "plt.text(2.75, 1.75, '?')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(0, 3)\n",
    "plt.title('Guesses of linear regression')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have designed the model. We can check how each data point is different from what the model predicts. We can obtain the expected y-coordinate $y_i^\\prime$, where $y_i^\\prime = b_0 + b_1 x_i$ for every data point with $x_i$.\n",
    "\n",
    "The difference between the actual y-coordinate $y_i$ and the expected y-coordinate $y_i^\\prime$ is then $y_i - b_0 + b_1 x_i$. The idea of *ordinary least squares* is to minimize the sum of $(y_i - b_0 - b_1 x_i)^2$.\n",
    "\n",
    "Let $f = \\sum{(y_i - b_0 - b_1 x_i)^2}$. We can use differentiation to find $b_0$ and $b_1$ such that $f$ is minimum.\n",
    "\n",
    "Set:\n",
    "- $\\frac{\\partial{f}}{\\partial{b_0}} = -2 \\sum{(y_i - b_0 -b_1 x_i)} = 0$\n",
    "- $\\frac{\\partial{f}}{\\partial{b_1}} = -2 \\sum{(y_i - b_0 -b_1 x_i) x_i} = 0$\n",
    "\n",
    "Then (after many steps...), we can obtain:\n",
    "- $b_0 = \\bar{y} - b_1\\bar{x}$ (hey, why?)\n",
    "- $b_1 = \\frac{\\sum{x_i(x_i - \\bar{y})}}{\\sum{x_i(x_i - \\bar{x})}} = \\frac{\\sum{(x_i-\\bar{x})(x_i - \\bar{y})}}{\\sum{(x_i - \\bar{x})(x_i - \\bar{x})}}$ (hey, 1$^{\\text{st}}$ why: why are the two expressions equivalent? 2$^{\\text{nd}}$ why: why do we prefer the latter?)\n",
    "\n",
    " \n",
    " Therefore:\n",
    " \n",
    " $\\text{slope of the line} = \\frac{\\sum{(x - \\bar{x})(y - \\bar{y})}}{\\sum{(x - \\bar{x})(x - \\bar{x})}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the linear regression line of a set of data\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "diff_x = data_x - xmean # if we do not use numpy array, we cannot do (vector - vector)\n",
    "diff_y = data_y - ymean\n",
    "\n",
    "# remember our model is in this form? y = b_0 + b_1 * x\n",
    "# slope is b_1 in our linear regression model\n",
    "\n",
    "slope = np.sum(diff_x * diff_y) / np.sum(diff_x * diff_x) \n",
    "\n",
    "# after we have calculated the slope, we can find the y-intercept b_0 by subsituting the slope and (x mean, y mean) into the formula\n",
    "y_intercept = ymean[0] - slope *xmean[0]\n",
    "\n",
    "\n",
    "# let's plot the graphs\n",
    "plt.figure(figsize=(10,8), dpi=100)\n",
    "plt.scatter(data_x, data_y,  color='black')\n",
    "\n",
    "# plot mean of x\n",
    "plt.plot(xmean, y, label='x mean') \n",
    "# plot mean of y\n",
    "plt.plot(x, ymean, label='y mean') \n",
    "\n",
    "# plot x errror\n",
    "for i in range(0, len(data)):\n",
    "    plt.plot([data_x[i], xmean[0]], [data_y[i], data_y[i]], '--')     \n",
    "# plot y errror\n",
    "for i in range(0, len(data)):\n",
    "    plt.plot([data_x[i], data_x[i]], [data_y[i], ymean[0]], '--') \n",
    "    \n",
    "x = np.linspace(0, 4, 4)\n",
    "y = [y_intercept + slope *x for x in x]\n",
    "plt.plot(x, y, label='Manual linear regression')\n",
    "\n",
    "# let try to use scikit-learn's linear regression\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(np.reshape(x, (-1, 1)), np.reshape(y, (-1, 1)))\n",
    "y_pred = regr.predict(np.reshape(x, (-1, 1)))\n",
    "plt.plot(x, y_pred, '--', label='Linear regression by scikit-learn')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(0, 3)\n",
    "plt.title('Linear regression');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use cases\n",
    "- Finding relationship between two continuous variables\n",
    "    - The higher the HP, the higher the Attack?\n",
    "- Prediction\n",
    "    - Who is the champion of World Cup 2018?\n",
    "- Classification\n",
    "    - See whether a data point is closer to a regression line A or another regression line B?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise **:\n",
    "- Try to use scikit-learn's linear regression to model the following data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(0, 0), (1, 1), (2, 2), (3, 3)] # where each data point is a pair of 2D coordinates. You may regard the first member as the x value, and the second member as the y value on a 2D Cartesian plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the intercept and coefficients? Are they what you expect?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
